{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a little notebook that can approximately say how much memory (and therefore how many GPUs) would be needed for a given scale of run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cells:  4.073333333333334e+08\n",
      "Number of particles:  3.055e+10\n",
      "Recommended number of GPUs:  214.0\n",
      "Recommended number of nodes: 36.0\n",
      "nx =  20000.0\n",
      "ny =  20367.0\n"
     ]
    }
   ],
   "source": [
    "# Ensure that you are only using 80% of GPU memory\n",
    "import numpy as np\n",
    "\n",
    "# Specify the bounds of the simulation in osiris units (c / wpe)\n",
    "x_bounds = [-3000,3000]\n",
    "y_bounds = [1890, 8000]\n",
    "\n",
    "dx = 0.3\n",
    "\n",
    "n_cells = (x_bounds[1]-x_bounds[0])*(y_bounds[1]-y_bounds[0])/(dx**2)\n",
    "print(\"Number of cells: \", np.format_float_scientific(n_cells))\n",
    "\n",
    "particles_per_cell = 25\n",
    "n_species = 3\n",
    "\n",
    "n_particles = n_cells * particles_per_cell * n_species # we need to allocate for twice as many particles\n",
    "n_bytes_particles = n_particles* 2 * 70 # maria says ~70 bytes per particle. I don't know if this is single or double precision\n",
    "\n",
    "max_bytes_per_GPU = 16e9/.8 # 80% of 16GB\n",
    "print(\"Number of particles: \", np.format_float_scientific(n_particles,3))\n",
    "\n",
    "print(\"Recommended number of GPUs: \", np.ceil(n_bytes_particles/max_bytes_per_GPU))\n",
    "print(\"Recommended number of nodes:\", np.ceil(n_bytes_particles/max_bytes_per_GPU/6))\n",
    "\n",
    "print(\"nx = \", round((x_bounds[1]-x_bounds[0])/dx,0))\n",
    "print(\"ny = \", round((y_bounds[1]-y_bounds[0])/dx,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, in order to use the tiles algorithm or the GPU algorithm, you need to specify the number of tiles you want in each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiles in each direction  631.0\n",
      "n_tiles =  1048576\n",
      "n_tiles_x = 1024, n_tiles_y = 1024\n",
      "Particles per tile:  9712.0\n",
      "You should set num_par_max to be 2x this:  19424.0\n"
     ]
    }
   ],
   "source": [
    "n_tiles_min = n_cells / 1024\n",
    "\n",
    "print(\"Number of tiles in each direction \", np.ceil(np.sqrt(n_tiles_min)))\n",
    "\n",
    "# Just keep typing in powers of two until you get n_tiles > n_tiles_min\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "while True:\n",
    "    n_tiles_x = 2**i\n",
    "    n_tiles_y = 2**j\n",
    "    n_tiles = n_tiles_x * n_tiles_y\n",
    "    if n_tiles > n_tiles_min:\n",
    "        break\n",
    "    i += 1\n",
    "    j += 1\n",
    "print(\"n_tiles = \", n_tiles_x * n_tiles_y)\n",
    "print(f\"n_tiles_x = {n_tiles_x}, n_tiles_y = {n_tiles_y}\")\n",
    "\n",
    "particles_per_tile = n_particles/n_tiles/n_species\n",
    "print(\"Particles per tile: \", np.ceil(particles_per_tile))\n",
    "print(\"You should set num_par_max to be 2x this: \", np.ceil(2*particles_per_tile))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FLASH-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
